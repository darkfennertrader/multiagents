{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a811e691-cb6a-4ca9-a082-0346126a6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import OPENAI_API_KEY, TAVILY_API_KEY, LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6955c36-51b3-4ef7-bf96-c0a5b09a6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "#url = \"https://docs.pyro.ai/en/stable/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join([doc.page_content for doc in d_reversed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcd977b-910c-49f6-8fb5-96e8981fac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba128f6-19c1-4143-ae89-ce3f35cd3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OpenAI\n",
    "\n",
    "# Grader prompt \n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "expt_llm = \"gpt-4-0125-preview\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
    "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a849d3cd-4df6-4dde-8ad9-50568ced2fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix=\"To build a Retrieval-Augmented Generation (RAG) chain in LCEL, you'll need to combine several components: a retriever to fetch relevant documents based on the input query, a prompt template to format the retrieved documents and the query for the language model, and finally, a language model to generate the response based on the formatted prompt. Here's how you can do it step by step:\", imports='from langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\\nfrom langchain_openai import OpenAIEmbeddings', code='# Initialize the in-memory document store with some texts\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\n\\n# Create a retriever from the document store\\nretriever = vectorstore.as_retriever()\\n\\n# Define a prompt template that includes the context and the question\\ntemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Define the output parser\\noutput_parser = StrOutputParser()\\n\\n# Combine the components into a RAG chain\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\n# Example invocation\\nresult = chain.invoke(\"where did harrison work?\")\\nprint(result)', description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b76383-540a-4d88-a0c8-4b6f34cf8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries \n",
    "    \"\"\"\n",
    "\n",
    "    error : str\n",
    "    messages : List\n",
    "    generation : str\n",
    "    iterations : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51218dfa-9128-4a43-9418-7db70ef512bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1e0eb-6a41-4e29-bcf5-bb707f4cf5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b149d-22e1-48ef-8763-87b71d6fbe04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
