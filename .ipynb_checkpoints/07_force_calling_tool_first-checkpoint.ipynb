{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f10faed-9903-428e-aa7b-cbdd94847eae",
   "metadata": {},
   "source": [
    "In this example we will build a chat executor that always calls a certain tool first. In this example, we will create an agent with a search tool. However, at the start we will force the agent to call the search tool (and then let it do whatever it wants after). This is useful when you want to force agents to call particular tools, but still want flexibility of what happens after that.\n",
    "\n",
    "This examples builds off the base chat executor. It is highly recommended you learn about that executor before going through this notebook. You can find documentation for that example here.\n",
    "\n",
    "Any modifications of that example are called below with MODIFICATION, so if you are looking for the differences you can just search for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ed06b2-2a29-418b-b2b5-e77a207f583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import OPENAI_API_KEY, TAVILY_API_KEY, LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1433da-7298-4642-bd12-3714b0fb8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e76c0f-1bec-4da6-8bf2-053c16960a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201b4a93-1513-4354-b1c9-bf77eec04617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42a39ec-2a4f-4b72-98d1-7bbb92cbebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(\n",
    "            last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        ),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c5b16-6493-40c4-88c9-3b2374a0cd69",
   "metadata": {},
   "source": [
    "MODIFICATION\n",
    "\n",
    "Here we create a node that returns an AIMessage with a tool call - we will use this at the start to force it call a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988520e7-39fd-4faa-9152-c8fd020286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new first - the first call of the model we want to explicitly hard-code some action\n",
    "from langchain_core.messages import AIMessage\n",
    "import json\n",
    "\n",
    "\n",
    "def first_model(state):\n",
    "    human_input = state[\"messages\"][-1].content\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                additional_kwargs={\n",
    "                    \"function_call\": {\n",
    "                        \"name\": \"tavily_search_results_json\",\n",
    "                        \"arguments\": json.dumps({\"query\": human_input}),\n",
    "                    }\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95470e4-3515-4f57-9b8a-f92ac55b6763",
   "metadata": {},
   "source": [
    "Define the graph\n",
    "We can now put it all together and define the graph!\n",
    "\n",
    "MODIFICATION\n",
    "\n",
    "We will define a first_agent node which we will set as the entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed187b-3694-4628-9b7a-84f0a8437be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bc22f-bcd3-4245-affa-5f2dd0acfa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43d159-52ec-4997-adab-d7d18c43e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
