{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae49ca5-fd16-46b9-9c66-e52a3fa9b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import OPENAI_API_KEY, TAVILY_API_KEY, LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72823d8-1ff2-4728-9ae9-670c1db1a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3ad7b-1f17-4916-babc-86a37e34a0fa",
   "metadata": {},
   "source": [
    "MODIFICATION\n",
    "\n",
    "We also want to define a response schema for the language model and bind it to the model as a function as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d19c27d-765b-4975-b5d0-b5572ce51503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Final response to the user\"\"\"\n",
    "\n",
    "    temperature: float = Field(description=\"the temperature\")\n",
    "    other_notes: str = Field(description=\"any other notes about the weather\")\n",
    "\n",
    "\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "functions.append(convert_to_openai_function(Response))\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a313b-56fc-4cc0-a392-6249df5999fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098365f8-c673-4aa6-a2fb-43158969c69d",
   "metadata": {},
   "source": [
    "### Define the nodes\n",
    "We now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this:\n",
    "\n",
    "The agent: responsible for deciding what (if any) actions to take.\n",
    "A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
    "We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).\n",
    "\n",
    "Conditional Edge: after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish\n",
    "Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next\n",
    "Let's define the nodes, as well as a function to decide how what conditional edge to take.\n",
    "\n",
    "MODIFICATION\n",
    "\n",
    "We will change the should_continue function to check what function was called. If the function Response was called - that is the function that is NOT a tool, but rather the formatted response, so we should NOT continue in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbd40f-38b3-4001-831b-8d50935ce6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395b405-d3d3-4621-9491-f2e628633121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435c943-4ac7-4dcf-9c82-4d89a605e917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8a6df-b354-4e7f-9abc-cd73134b7422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
