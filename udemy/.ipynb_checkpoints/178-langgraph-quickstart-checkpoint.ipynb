{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4c3536-8304-4044-8092-d50ec7ea308b",
   "metadata": {},
   "source": [
    "# LangGraph Quickstart\n",
    "* As usual, we do not like the way the LangGraph documentation as it is presented by the LangChain team. The style is too technical and the naming conventions are sometimes confusing.\n",
    "* The complexity of LangGraph is the reason why we are seeing the rapid growth of other simpler frameworks like CrewAI, which at the end of the day are just \"LangGraph with a touch of simplicity\".\n",
    "* The best way to learn about LangGraph will be to see it at work in the next basic project. Before doing that, in this lecture we will make a quick review of the LangGraph documentation as it is today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09905f1-6c6a-4d84-bfb4-a5719e7907ea",
   "metadata": {},
   "source": [
    "## LangGraph Documentation\n",
    "* [LangGraph Documentation](https://python.langchain.com/docs/langgraph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1447dce-af44-4f02-aece-43ec81083126",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* Framework to develop Multi-Agent LLM Apps.\n",
    "* Python and Javascript versions.\n",
    "* Main uses:\n",
    "    * add cycles to LLM App.\n",
    "    * add persistence to LLM App."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0788a36-723d-4705-90be-a9499d741284",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4594b5f-3900-4203-9f94-adfd572b65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe08c9-3217-42c0-b44b-48c3bf92967e",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "* Graph: agent, multi-agent.\n",
    "* Nodes: actions.\n",
    "* Edges: node connections.\n",
    "* State: memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e22b2e-33a5-4aeb-85ac-9bf07016eccc",
   "metadata": {},
   "source": [
    "## Basic agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c4738-f7e8-4efc-bf76-db650bf97614",
   "metadata": {},
   "source": [
    "#### Recommended: create new virtualenv\n",
    "* pyenv virtualenv 3.11.4 your_venv_name\n",
    "* pyenv activate your_venv_name\n",
    "* pip install jupyterlab\n",
    "* jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64a634-3ede-4138-aff5-26e35339bead",
   "metadata": {},
   "source": [
    "#### .env File\n",
    "Remember to include:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f3dab8-fbcf-4bb8-8421-528ca4cfff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88be996-66df-4422-8263-cf6c40f5e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd3236-6b29-43ed-b826-47bf6ab84f30",
   "metadata": {},
   "source": [
    "* First, we initialize our model and an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24315ce5-19b8-4d98-9b7b-94392e87637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "agent = MessageGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42524d91-a605-431b-9394-0350282796a4",
   "metadata": {},
   "source": [
    "* Next, we add a single node to the agent, called \"node1\", which simply calls the LLM with the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61acd935-b6f5-4a9f-bbb8-5d26474bfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.add_node(\"node1\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30fb73-a829-4253-8729-9043e6cc1b7c",
   "metadata": {},
   "source": [
    "* We add an edge from this \"node1\" node to the special string END (`__end__`). This means that execution will end after the current node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439c1d4b-c51f-4890-a6d5-b6e91b48df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.add_edge(\"node1\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109008c-e113-4537-b5d2-56589d66993d",
   "metadata": {},
   "source": [
    "* We set \"node1\" as the entry point to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7b03ae-8d23-4511-bdc8-c814dc9d8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_entry_point(\"node1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919eeab-9e23-4d9f-a328-9e750511a910",
   "metadata": {},
   "source": [
    "* We compile the agent, translating it to low-level pregel operations ensuring that it can be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e9d27b-055c-4355-a0bf-201b20f8471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_agent = agent.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c562a-25c0-410c-82f4-293c510a9305",
   "metadata": {},
   "source": [
    "* We can run the agent now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854ad47f-f527-4f69-92c1-262de712c35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 1 + 1?', id='93e77112-1bf7-49ae-84bf-82bb73ed77b5'),\n",
       " AIMessage(content='1 + 1 equals 2.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f51a1195-37d8-4ceb-ab4a-170e9597d760-0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent.invoke(HumanMessage(\"What is 1 + 1?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4844b88-9188-4329-b67c-052da43780f5",
   "metadata": {},
   "source": [
    "When we execute the agent:\n",
    "* LangGraph adds the input message to the  state, then passes the state to the entrypoint node, \"node1\".\n",
    "* The \"node1\" node executes, invoking the chat model.\n",
    "* The chat model returns an AIMessage. LangGraph adds this to the state.\n",
    "* Execution progresses to the special END value and outputs the final state.\n",
    "* And as a result, we get a list of two chat messages as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3d3b4-296d-4588-9ebb-fcac4101f827",
   "metadata": {},
   "source": [
    "## Agent with a router and conditional edges\n",
    "* Let's allow the LLM to conditionally call a \"multiply\" node using tool calling.\n",
    "* We'll recreate our previous agent with an additional \"multiply\" tool that will take the result of the most recent message, if it is a tool call, and calculate the result.\n",
    "* We will bind the multiply tool to the OpenAI model to allow the llm to optionally use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00286c87-0cab-403a-8b02-7d1d2a55c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab82ae3-b8d4-47ed-9f0c-733c199e517f",
   "metadata": {},
   "source": [
    "Let's create the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b038c5e-20af-4f9e-be2b-a3799975930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_conditional_edges = MessageGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77add60-dfb1-4a98-a492-67b51cffd537",
   "metadata": {},
   "source": [
    "Let's create the first node of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a8da67-6eb7-496b-b481-4cdaf57e7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_conditional_edges.add_node(\"node1\", llm_with_tools)\n",
    "agent_with_conditional_edges.set_entry_point(\"node1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20d90c-53dd-4b3c-b957-63d581edb719",
   "metadata": {},
   "source": [
    "Let's create the second node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ab0e41-ff15-4b4e-9e5d-19e5620fe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode([multiply])\n",
    "agent_with_conditional_edges.add_node(\"multiply\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a1020-b723-4964-b247-d38f63a257c2",
   "metadata": {},
   "source": [
    "Let's create the edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3220199e-b603-4877-b565-9145a71e5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_conditional_edges.add_edge(\"multiply\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c673c-11e5-4cc9-b734-90a43b243e51",
   "metadata": {},
   "source": [
    "Using conditional edges, which call a function on the current state and routes execution to a node the function's output:\n",
    "* If the \"node1\" node returns a message expecting a tool call, we want to execute the \"multiply\" node.\n",
    "* If not, we can just end execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f94c8e55-2403-4c09-afe7-2f6cf821be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "def router(state: List[BaseMessage]) -> Literal[\"multiply\", \"__end__\"]:\n",
    "    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "agent_with_conditional_edges.add_conditional_edges(\"node1\", router)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cceb0-ed58-4e55-8468-5c15015b706e",
   "metadata": {},
   "source": [
    "Now all that's left is to compile the graph and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeaee428-96c2-4a9c-b41b-2baa3a02d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_agent_with_conditional_edges = agent_with_conditional_edges.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd3abd-60e3-429e-991c-1fa163255114",
   "metadata": {},
   "source": [
    "Math-related questions are routed to the multiply tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40cbba33-d38e-478f-a1d1-85d46d12b0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 123 * 456?', id='778697ff-aa41-4035-ae6f-cba4be82434f'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gC1Y2loZduDpioi9KoMniWKA', 'function': {'arguments': '{\"first_number\":123,\"second_number\":456}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 69, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a58e9110-cc5d-406d-ad41-9953dbfc38b6-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_gC1Y2loZduDpioi9KoMniWKA'}]),\n",
       " ToolMessage(content='56088', name='multiply', id='a4a4a3bf-f401-4e17-9964-8e810465952d', tool_call_id='call_gC1Y2loZduDpioi9KoMniWKA')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent_with_conditional_edges.invoke(HumanMessage(\"What is 123 * 456?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50de57-7c4a-4e14-9225-ec83cee53877",
   "metadata": {},
   "source": [
    "While conversational responses are outputted directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de5cc98d-a23c-4938-b805-a5af73c61525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is your name?', id='c4fa88f6-ae86-4e10-9369-80ca8933a851'),\n",
       " AIMessage(content='My name is Assistant. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 66, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-89710e80-33b0-4289-8f48-97d81645e314-0')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_agent_with_conditional_edges.invoke(HumanMessage(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932c7a1-fc56-4e18-b3ec-347bd31aefac",
   "metadata": {},
   "source": [
    "## Agent with cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ebf27-f9fa-4b5e-b64d-4814e2c5db5c",
   "metadata": {},
   "source": [
    "* We will use Tavily as online search tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8a885d2-a923-41d6-b459-65db9a2ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba547f94-e46f-4ca7-8a78-259e16baf340",
   "metadata": {},
   "source": [
    "* Remember to add the tavily api key in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "060717f1-4f9d-46db-9afa-8d570074c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "online_search_tool = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64db46-2fc5-4908-88ab-d90bb8307a98",
   "metadata": {},
   "source": [
    "* We can now wrap these tools in a simple LangGraph ToolNode. This class receives the list of messages (containing tool_calls), calls the tool(s) the LLM has requested to run, and returns the output as new ToolMessage(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "508e0ebd-ff0f-44e3-b422-1f88048ab8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node_with_online_search = ToolNode(online_search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4326e4-0c81-4912-a021-4bcf00c34f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f88fb2-ef22-46cd-ae5c-9d7fc19df916",
   "metadata": {},
   "source": [
    "* Let's make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the bind_tools() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f98e968-5839-46f4-bf3f-347fee3a374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_online_search_tool = llm.bind_tools(online_search_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47331a3-a735-4e74-b87b-767db304275c",
   "metadata": {},
   "source": [
    "* Let's now define the state of the agent.\n",
    "* Each node will return operations to update the state.\n",
    "* For this example, we want each node to just add messages to the state. Therefore, in this case the state of the agent will be a list of messages. In other projects, the state can be any type.\n",
    "* We will use a TypedDict with one key (messages) and annotate it so that we always add to the messages key when updating it using the `is always added to` with the second parameter (operator.add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19e0fd42-f5c9-4964-b3d5-15070971b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Add-don't-overwrite.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The `add_messages` function within the annotation defines\n",
    "    # *how* updates should be merged into the state.\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3536d0-4a9c-43d2-a3c0-299c3d14d6f0",
   "metadata": {},
   "source": [
    "* Let's now define the nodes of this agent:\n",
    "    * the node responsible for deciding what (if any) actions to take.\n",
    "    * if the previous node decides to take an action, this second node will then execute that action calling the online search tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fbd25-767e-45f9-b522-655736102e93",
   "metadata": {},
   "source": [
    "* We will also define the edges that will interconnect the nodes.\n",
    "* One will be a Conditional Edge. After the node1 is called, the llm will dedice either:\n",
    "    * a. Run the online search tool (node2), OR\n",
    "    * b. Finish\n",
    "\n",
    "* The second will be a normal Edge: after the online search tool (node2) is invoked, the graph should always return to the node1 to decide what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d02e4d06-06c5-44d0-82c7-a85e9007c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState) -> Literal[\"node2\", \"__end__\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"action\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"node2\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    response = llm.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51b42559-9395-4b57-8cb1-54b7e5f6aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"node1\", call_model)\n",
    "workflow.add_node(\"node2\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `node1`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"node1\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `node1`.\n",
    "    # This means these are the edges taken after the `node1` node is called.\n",
    "    \"node1\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `online_search_tool` to `node1`.\n",
    "# This means that after `online_search_tool` is called, `node1` node is called next.\n",
    "workflow.add_edge('node2', 'node1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaae53-e9e1-4d7d-b028-0e5481221a9a",
   "metadata": {},
   "source": [
    "* Now we can compile it and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29ac4e45-66ee-481e-8819-387be5e42dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_cycles = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df52cee8-2669-4928-b12e-59bd75868b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf'),\n",
       "  AIMessage(content='The weather in San Francisco can vary greatly throughout the year. In general, the city has a mild climate with cool, foggy summers and wet winters. The average temperature in the summer months is around 60-70째F (15-21째C), while in the winter months it is around 50-60째F (10-15째C). It is always a good idea to check a reliable weather forecast for the most up-to-date information.', response_metadata={'finish_reason': 'stop'}, id='run-7c928a12-baca-4bf0-945d-b16acdf8f143-0')]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "agent_with_cycles.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed12b3-53b0-423e-9521-0fbefdc5ece6",
   "metadata": {},
   "source": [
    "## Additional documentation\n",
    "* [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/).\n",
    "* [LangGraph How-To Guides](https://langchain-ai.github.io/langgraph/how-tos/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972089c5-b1c2-4938-82f9-5d7333d79f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
